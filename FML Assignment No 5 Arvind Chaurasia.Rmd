---
title: "FML Assignment No 5 Arvind Chaurasia"
output: html_document
date: "2023-11-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
***This assignment was completed and uploaded by Arvind Chaurasia. (Eamil id: achauras@kent.edu)***

---

**Question**
The dataset Cereals.csv includes nutritional information, store display, and consumer ratings for 77 breakfast cereals.

Data Preprocessing. Remove all cereals with missing values.

● Apply hierarchical clustering to the data using Euclidean distance to the normalized
measurements. Use Agnes to compare the clustering from single linkage, complete
linkage, average linkage, and Ward. Choose the best method.

● How many clusters would you choose?

● Comment on the structure of the clusters and on their stability. Hint: To check stability,
partition the data and see how well clusters formed based on one part apply to the other
part. To do this:  ● Cluster partition A   ● Use the cluster centroids from A to assign each record in partition B (each record is assigned to the cluster with the closest centroid). ● Assess how consistent the cluster assignments are compared to the
assignments based on all the data.

● The elementary public schools would like to choose a set of cereals to include in their
daily cafeterias. Every day a different cereal is offered, but all cereals should support a
healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.”
Should the data be normalized? If not, how should they be used in the cluster analysis?
```{r}

# R packages for data manipulation and visualization.
suppressMessages(library(tidyverse))  

# This provides various clustering algorithms.
library(cluster)  

# This will be used for visualizing the results of clustering analysis. 
suppressMessages(library(factoextra))

# This will be used for working with dendrograms.
suppressMessages(library(dendextend))  

# For creating color dendrograms.
library(sparcl)  

# Provide additional functions for creating scatterplot matrices and other complex visualizations.
suppressMessages(library(GGally))     
```

Now we will load our dataset and will check for the cerels with missing values. 
```{r}
data_frame <- read.csv("/Users/arvindkc91/Desktop/Rhistory/Cereals.csv")
ref_data <-read.csv("/Users/arvindkc91/Desktop/Rhistory/Cereals.csv")

#Now we will see the staructure of our dataset using str() function. 
str(data_frame)

# Further we will count missing values for each column or attribute in our data frame. 
missing_values_per_column <- colSums(is.na(data_frame))

# Lets's count total missing values in the entire dataset.
total_missing_values <- sum(is.na(data_frame))

# Displaying our results of missing value.
print("Missing values per column:")
print(missing_values_per_column)

print(paste("Total missing values in the dataset:", total_missing_values))

```

From the above step it is evident that there are 4 missing values. In order to remove 4 missing values in our data set that we will be using for clustering.  
```{r}
#Here we are using na.omit() function, to remove the rows containing the missing value. 
data_frame <- na.omit(data_frame)
ref_data<-na.omit(ref_data)

#In order to check, let's see how many missing value we still have. This should give us zero. 
sum(is.na(data_frame))
```
In the above we have got zero, which means that there are no missing values. 

Now, row names will be useful for indexing and referencing specific later while labeling our cluster.  
```{r}
rownames(data_frame) <- data_frame$name
rownames(ref_data) <- ref_data$name
```

Now we will remove the Celers name from our data set. The coloumn name is 'name'.
```{r}
data_frame$name = NULL
ref_data$name = NULL
```

The data must be scaled, before measuring any type of distance metric as the variables with higher ranges will significantly influence the distance
```{r}
# Display column names and data types before scaling
str(data_frame[, 3:15])

#Now scaling our data. 
data_frame <- scale(data_frame[,3:15])

#Now using head() function to see the scaled value of first 6 coloumn. 
head(data_frame)
```

Now we will be using Euclidean distance to measure the dissimilarity (distance). After that we are using the hclust() to perform the clustering and their after we are plotting it. 
```{r}
# dist() function will give the dissimilarity matrix()
dis <- dist(data_frame, method = "euclidean")

# Let's perform hierarchical clustering using complete Linkage method. Here in the hclust() function we are required to provide the dissimilarity matrix(). 
hcc <- hclust(dis, method = "complete" )

# Let's plot the dendrogram using the result obtained in the above step. 
plot(hcc, cex = 0.6, hang = -1)
```

As asked in the question we will be using agnes() compare the clustering from single linkage, complete
linkage, average linkage, and Ward.
```{r}
#using library cluster() to perform the clustering using agnes() 
library(cluster)

#performing the clustering using the single linkage method. 
hcs <- agnes(data_frame, method = "single")

#Plotting the single linkage dendrigram
pltree(hcs, cex = 0.6, hang = -1, main = "Single Linkage Dendrogram of Agnes")
```

Now Performing Hierarchical Clustering using Average Linkage method.
```{r}
#Average linkage method.
hca <- agnes(data_frame, method = "average")

#Plotting the graph. 
pltree(hca, cex = 0.6, hang = -1, main = "Average Linkage Dendrogram of agnes")
```

Similarly, we will repeat this method for complete linkage method.
```{r}
hcc <- agnes(data_frame, method = "complete")
pltree(hcc, cex = 0.6, hang = -1, main = "Complete Linkage Dendrogram of agnes")
```

Now doing the same process for Ward Linkage Dendrogram. 
```{r}
hcw <- agnes(data_frame, method = "ward")
pltree(hcw, cex = 0.6, hang = -1, main = "Ward Linkage Dendrogram of agnes")
```

In order to choose the best method for us now we will be calculating the agnes coefficient of all the four linkage methods. As we know that the higher agnes co-efficient value means better seperation between the clusters. 

```{r}
# Four linkage methods we used.
a <- c( "average", "single", "complete", "ward")
names(a) <- c( "average", "single", "complete", "ward")

# Let's define our function to Compute Agglomerative Coefficient for each method.
ac <- function(x) {
  agnes(data_frame, method = x)$ac
}

#Lets summaries the value we obtained above.
map_dbl(a, ac) 
```
As we have already discussed the higher is the better, hence,  ward linkage is the best with agglomerative method for us for this data set. 

---

Now we will do the rectangular clustering vizulailization. rect.hclust() will be used to draw the colorful rectangle around the cluster. Here we are dividing it in 6 clusters. 
```{r}
#First let us create the distance matrix, to use in clustering.
distance <- dist(data_frame, method = "euclidean")

# Now using Ward's linkage method. 
hcw_cut <- hclust(distance, method = "ward.D2" )

#Now plotting it in a visually appealing way. 
plot(hcw_cut, cex=0.6 )

rect.hclust(hcw_cut,k=6,border = 1:6)
```

Now cutting the cluster into 6 part and seeing how many of them are in each.
```{r}
clus_gr <- cutree(hcw_cut, k = 6)
table(clus_gr)
```

Now we are creating one co relation matrix visualization, which well help to understand the relation between two variables in a better way. 
```{r}
#install.packages("GGally")
ref_data %>% 
  select(calories, protein, fat, sodium, fiber, carbo, sugars, potass,vitamins,rating) %>% 
  ggcorr(palette = "YlOrRd", label = TRUE, label_round =  1)
```
From the above we can see that fiber and potass has very strong relationship. Similarly, we can interpret other relationship. 

We have will be using the pvclust package to perform hierarchical clustering with bootstrapped p values,a measure of how robust the clusters are to variations in the dataset. Higher p-values indicate more stable clusters, while lower p-values suggest potential instability or sensitivity to random variations in the data.

To visualizes the dendrogram with p values, and highlights groups that are deemed highly supported by rectangles around them. This is a useful approach for assessing the stability and reliability of clusters obtained through hierarchical clustering. 
```{r}
# Ward Hierarchical Clustering with Bootstrapped p values

#install.packages("pvclust")
library(pvclust)

fit.pv <- pvclust(data_frame, method.hclust="ward.D2",method.dist="euclidean")

# plotting a dendogram using p values.
plot(fit.pv) 

# Now adding rectangles around groups highly supported by the data.
pvrect(fit.pv, alpha=.95)
```


The stability of each group in the original grouping is calculated by finding the average Jaccard coefficient across multiple trials. Generally, if a group's stability is below 0.6, it's not very reliable. A stability value between 0.6 and 0.75 means the group shows a pattern in the data, but we're not very sure about which items should be in the same group. Groups with a stability value above 0.85 are very reliable. 

Now, let's simplify the three points: 

Try to make the average Jaccard coefficient for each group as high as possible. 

Aim to have as few groups falling apart (dissolving) as possible. 

Aim to find as many groups as you initially thought there would be, and get them as close to that number as you can.
```{r}
#Loading the necessary library
library(fpc)
library(cluster)

#Defining the number of clusters, we have obtained in our previous dteps.
kbest.p<-6

#Performing the cluster bootstarping 
cboot.hclust <- clusterboot(data_frame,clustermethod=hclustCBI,method="ward.D2", k=kbest.p)

#Getting the summary of the boot starp clustering. 
summary(cboot.hclust$result)

#Retrieving the cluster membership and displaying the first few rows of the data frame showing the cluster memberships.
groups<-cboot.hclust$result$partition
head(data.frame(groups))

#Cluster Stability Measure. 
#Provides a vector of cluster stabilities.
cboot.hclust$bootmean

#The count of how many times each cluste was dissolved during the bootstrap process. clusterboot() runs 100 bootstrap iterations by default. 
cboot.hclust$bootbrd
```
Looking at the results, we can confidently say that clusters 1 and 3 are very stable, meaning they show clear and consistent patterns. 

However, clusters 4 and 5 are a bit tricky – we can see some patterns, but we're not entirely sure which points should be grouped together. 

As for clusters 2 and 5, they seem a bit shaky and uncertain, indicating some instability in their composition.


```{r}
# cutree() function used to cut dendrogram at specified height or number of cluster using k.
groups <- cutree(hcw_cut, k = 6)

# Now we will define a function to print the cluster information. 
print_clusters <- function(labels, k) {
for(i in 1:k) {
print(paste("cluster", i))
print(ref_data[labels==i,c("calories","protein","fiber","potass","vitamins","rating")])
}
}

#Now printing the data from each cluster.
print_clusters(groups, 6)

```

```{r}
# Now we will define a function to print the cluster information.
print_clusters <- function(labels, k, data) {
  for (i in 1:k) {
    cat("Cluster", i, ":\n")
    cluster_data <- data[labels == i, c("calories", "protein", "fiber", "potass", "vitamins", "rating")]
    
    # Print mean of each column for each cluster
    cat("Mean Calories:", mean(cluster_data$calories), "\n")
    cat("Mean Protein:", mean(cluster_data$protein), "\n")
    cat("Mean Fiber:", mean(cluster_data$fiber), "\n")
    cat("Mean Potassium:", mean(cluster_data$potass), "\n")
    cat("Mean Vitamins:", mean(cluster_data$vitamins), "\n")
    cat("Mean Rating:", mean(cluster_data$rating), "\n\n")
  }
}

# Now printing the mean from each cluster.
print_clusters(groups, 6, ref_data)


```




For this question we will assume that cereal having a overall better value in "calories", "protein", "fiber", "potass", "vitamins", and "rating" is a healthy and based upon this we will identify healthy cereals cluster. 

As we can see that only cluster 1 and cluster 6 have the mean rating above 50%. For our analysis we are considering the a threshold of 50%. 

The other factors here are comparable and some of them are higher in one cluster and some of them are lower in other. 

Based on the specific requirement school may choose either of the cluster group. Also we may consider the number of options available in that particular cluster. 

Data normalization should be done based upon the nature of the data set. Hierarchical clustering with certain distance measures, can handle different scales without much difficulties. If we are selecting features which have similar scale, we don't need to normalize. In all other scenario data normalization would be done to get equal weightage of all features. 

---









